---
title: "Comparing Static Website Generators Over Time"
author: "Peter Baumgartner"
date: "`r Sys.Date()`"
output:
  html_notebook:
    fig_caption: yes
    number_sections: yes
    pandoc_args: --number-offset=0,0
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
  pdf_document:
    pandoc_args: --number-offset=0,0
    toc: yes
    toc_depth: '4'
    latex_engine: xelatex
  github_document:
    toc: yes
    toc_depth: 4
  html_document:
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    pandoc_args: --number-offset=0,0
    toc: yes
    toc_depth: 4
---

# Setup

```{r label = "global-options", highlight=TRUE}
knitr::opts_chunk$set(
        message = F,
        error = F,
        warning = F,
        comment = NA,
        highlight = T,
        prompt = T
        )

### install and load some important packages
### https://github.com/tidyverse/tidyverse
if (!require("tidyverse"))
        {install.packages("tidyverse", repos = 'http://cran.wu.ac.at/')
        library(tidyverse)}

### above command installed and loaded the core tidyverse packages:
# ggplot2:    data visualisation
# tibble:     a modern take on data frames
# tidyr:      data tidying
# readr:      data import (csv, tsv, fwf)
# purrr:      functional R programming
# dplyr:      data (frame) manipulation
# stringr:    string manipulation
# forcats:    working with categorial varialbes
# tidyselect: backend for the selecting functions of the 'tidyverse'. (?, new?)


### My personal reminder for other important packages:
### Working with times:
# hms, for times.

# lubridate, for date/times.
if (!require("lubridate"))
        {install.packages("lubridate", repos = 'http://cran.wu.ac.at/')
        library(lubridate)}

### Importing other types of data:
# feather, for sharing with Python and other languages.
# haven, for SPSS, SAS and Stata files.
# httr, for web apis.
# jsonlite for JSON.
# readxl, for .xls and .xlsx files.


# rvest, for web scraping.
if (!require("rvest"))
        {install.packages("rvest", repos = 'http://cran.wu.ac.at/')
        library(rvest)}


# xml2, for XML.
if (!require("xml2"))
        {install.packages("xml2", repos = 'http://cran.wu.ac.at/')
        library(xml2)}

### Modelling
# modelr, for modelling within a pipeline
# broom, for turning models into tidy data


### Special packages for this article
if (!require("wayback"))
        {remotes::install_github("hrbrmstr/wayback", build_vignettes = TRUE)
        library(tidyverse)}
```

# Preliminaries

## Does the Internet Archive have my research URL cached?

Using `archive_available(url, timestamp)`: Timestamp is optional. 
The function returns a tibble with one observation and 5 variables:

```{r url-cached}
staticgen_avail <- archive_available("https://www.staticgen.com/")
staticgen_avail
```

## Retrieve site mementos from the Internet Archive

Mementos are prior versions of web pages that have been cached from web crawlers. They can be found in web archives (such as the Internet Archive) or systems that support versioning such as wikis or revision control systems.

With `get_mementos(url, timestamp = format(Sys.Date(), "%Y"))` we will receive a short list of relevant links to the archived content. The function returns the four link relation types as in the [Request for Comment for the Memento framework](https://mementoweb.org/guide/rfc/#Link-Header-Relation-Types) outlined.

1. Link Relation Type "original"
2. Link Relation Type "timemap"
3. Link Relation Type "timegate"
4. Link Relation Type "memento"

Besides these 4 main types of link relations the function provides also the first, previous and last available memento, which is normally identical with the memento link relation type. In addition to the two columns `link' and `rel` there is a third one `ts`, containing the time stamps (empty for the first three link relation types). The return value in total is a tibble with 7 observations (rows) and three columns.

```{r get-mementos}
staticgen_mntos <- get_mementos("https://www.staticgen.com/")
staticgen_mntos
```

## Get the point-in-time memento crawl list

Providing an URL in the search field of the Wayback Machine results in the interactive browser version into the [calender view](https://web.archive.org/web/*/https://www.staticgen.com) where the dates with archived content are blue or green (redirected URL) circled. the bigger the circles the more snapshots were archived on these dates.

We get these dated crawl list with the second observation of the `get_mementos` function.

```{r get-timemap}
staticgen_tm <- get_timemap(staticgen_mntos$link[2])
staticgen_tm
```

In addition to the 476 captures of the interactive browser version there are four more rows (480), relating to the four link relation types mentioned above. The last line is empty.

## Summary: Putting all together

We can put together all three preliminay steps into a function `get_rawcrawl(url)`. This functions gets an URL and returns a list of all archived versions for this URL.

1. Check if for the URL exists an archived version. If not: stop execeution.
2. If exists an archived version, then retireve mementos for this url from the Internet Archive.
3. Get the point-in-time memento crawl list for this URL

```{r get-raw-crawl}
get_rawcrawl <- function(url) {
  if(is_url(url)) {
    url_archived <- archive_available(url) 
    if (url_archived$available) {
      mementos <- get_mementos(url)
      time_map <- get_timemap(mementos$link[2])
      return(time_map)
    } else {
      return(paste0("There exists no archive of '", url, "'."))
    }
  } else {
    stop("The functions needs a valid URL format: 'http://' or 'https://'")
  }
}

is_url <- function(s) {
  class(s) == "character" && (substr(s,1,7) == "http://" || substr(s,1,8) == "https://")
}

sg_rawcrawl <- get_rawcrawl("https://www.staticgen.com/")
saveRDS(sg_rawcrawl, file = "data/sg_rawcrawl.rds")
sg_rawcrawl
```

# Tidy data

At first we have to clean up our data frame of URLs to crawl. Tidying the timemap data frame is a multipe step procedure:

+ Clean up so that only memento links remain
+ Delete unnecessary rows `type` and `from`.
+ Convert row `datetime` from class 'character' to class 'datetime'.
+ Delete duplicate datetime records. (Sometimes there are more than one capture taken at the same day, refering to the URL and the port used.)
+ Filter rows with an algorithm, so that only those mementos remain which are suitable for the comparison analysis. For instance: Take the first memento for every year, or every month etc.

The last step is special as it requires a decision by the author or analyist of the data. 

## General data cleaning

```{r get-clean-crawl}
get_cleancrawl <- function(df) {
  df$datetime <- as.POSIXct(df$datetime, format = "%a, %d %b %Y")
  df_crawl <- df %>% 
      filter(rel == "memento") %>%
      select(link, datetime) %>%
      distinct(datetime, .keep_all = TRUE)  # delete duplicate datetime
  return(df_crawl)
}

sg_rawcrawl <- readRDS("data/sg_rawcrawl.rds")
sg_cleancrawl <- get_cleancrawl(sg_rawcrawl)
saveRDS(sg_cleancrawl, file = "data/sg_cleancrawl.rds")
sg_cleancrawl
```

## Filter crawl list

This is a more complicated functions as it provides several posssiblities:

1. Limit comparison period:
    1. Choose start of comparison period by row number.
    2. Choose start of comparison period by (nearest) date.
    3. Choose end of comparison period by row number.
    4. Choose end of comparison period by (nearest) date.
2. Adding URLs in any case, independent of the chosen filter algorithm:
    1. Add URL of the last memento in any case, independent of the filter option.
    2. Add URL of the live web site with datetime of today to the end of the data frame.
3. Mode of calculation of the chosen algorithm:
    1. Take always first entry of the chosen period.
    2. Take always last entry of the chosen period.
    3. Take datetime of your first chosen memento to calculate the period.
4. Filter rows with one of the following options:
    1. **Year:** One URL to crawl for every year.
    2. **Half year (six months):** One URL to crawl for every 6 months.
    3. **Quarter (three months):** One URL to crawl for every quarterly period. 
    4. **Month:** One URL to craw for every month.
    5. **Number:** Filter <number> URLs with roughly equidistance of time.

Limiting the comparison period is useful for several reasons:
    + To limit the time period for the data analysis.
    + Ignore the first mementos of an archived web site because they have not enough information.
    + Generate various collections of mementos, depending of their different structure to crawl.

At the moment I have only implemented: 1.1, 2.2, 3.1, and 4.1.

```{r filter-craw-list}
get_crawllist <- function(
  df, start=1, end=nrow(df),
  last_mnto=FALSE, live_url="",
  choose_mnto='first',
  filter_mntos='year') {
  
  if (start != 1) {df <- tail(df, -start)}
  
  if (filter_mntos == 'year') {
    crawl_list <- df %>%
    mutate(year = year(datetime)) %>%
    # http://bit.ly/2K0oho0
    group_by(year) %>%
    filter(datetime == min(datetime)) %>% 
    ungroup()
  }
  if (!live_url == '') {
    live_wbpg <- tibble(link = live_url, 
                        datetime = as_datetime(today()), 
                        year = as.numeric(2019))
    crawl_list <- data.frame(rbind(crawl_list, live_wbpg))
  }
  return(crawl_list)
}


sg_cleancrawl <- readRDS("data/sg_cleancrawl.rds")
# The first 12 mementos have a different web site structure
# I am loosing about 8 month for the comparison
sg_crawllist <- get_crawllist(sg_cleancrawl, start = 13, live_url = "https://www.staticgen.com/")
saveRDS(sg_crawllist, file = "data/sg_crawllist.rds")
sg_crawllist

sg_wbpg <- lapply(sg_crawllist$link, read_html)
saveRDS(sg_wbpg, file = "data/sg_wbpg.rds")
sg_wbpg

```


<!-- ```{r staticgen-filter} -->
<!-- # clean timemap: take only the first entry of every year) -->
<!-- staticgen_tm <- readRDS("data/staticgen_tm.rds") -->
<!-- staticgen_tm$datetime <- as.POSIXct(staticgen_tm$datetime, format = "%a, %d %b %Y") -->
<!-- staticgen_tm <- staticgen_tm %>% -->
<!--   filter(rel == "memento") %>% -->
<!--   select(link, datetime) %>% -->
<!--   mutate(year = year(datetime)) %>% -->
<!--   distinct(datetime, .keep_all = TRUE) %>%  # delete duplicate datetime -->
<!--   # http://bit.ly/2K0oho0 -->
<!--   group_by(year) %>% -->
<!--   filter(datetime == min(datetime)) -->
<!-- ``` -->


<!-- ## Reading the mementos into memory -->

<!-- ```{r read-memento} -->
<!-- mem <- read_memento(staticgen_tm$link[479]) -->
<!-- readr::write_lines(mem, path = "data/staticgen_tm.txt") -->
<!-- ``` -->


<!-- ```{r load-data} -->
<!-- staticgen_avail <- readRDS("data/staticgen_avail.rds") -->
<!-- staticgen_mntos <- readRDS("data/staticgen_mntos.rds") -->
<!-- staticgen_tm <- readRDS("data/staticgen_tm.rds") -->
<!-- ``` -->


<!-- ```{r read-web-achive-10} -->
<!-- staticgen_arch10 <- read_html(staticgen_tm$link[10]) -->
<!-- staticgen_arch10 %>%  -->
<!--   html_nodes("h4") %>%  -->
<!--   html_text() -->
<!-- ``` -->

<!-- ```{r read-web-achive-15} -->
<!-- staticgen_arch15 <- read_html(staticgen_tm$link[15]) -->
<!-- staticgen_arch15 %>%  -->
<!--   html_nodes("h4") %>%  -->
<!--   html_text() -->
<!-- ``` -->


<!-- ```{r read-web-achive-17} -->
<!-- staticgen_arch17 <- read_html(staticgen_tm$link[17]) -->
<!-- staticgen_arch17 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- ``` -->


<!-- ```{r scrap-values17} -->
<!-- staticgen_values17 <- staticgen_arch17 %>%  -->
<!--   html_nodes(".stat") %>%  -->
<!--   html_text() -->
<!-- ``` -->


<!-- ```{r read-web-achive-20} -->
<!-- staticgen_arch20 <- read_html(staticgen_tm$link[20]) -->
<!-- staticgen_arch20 %>%  -->
<!--   html_nodes("h4") %>%  -->
<!--   html_text() -->
<!-- ``` -->


<!-- ```{r scrap-name} -->
<!-- staticgen_wbpg <- read_html("https://www.staticgen.com/") -->
<!-- staticgen_name <- staticgen_wbpg %>%  -->
<!--   html_nodes("h4") %>%  -->
<!--   html_text() -->
<!-- ``` -->

<!-- ```{r scrap-values} -->
<!-- staticgen_values <- staticgen_wbpg %>%  -->
<!--   html_nodes(".OpenSourceStat-sc-1jlkb1d-2") %>%  -->
<!--   html_text() -->
<!-- ``` -->

<!-- ```{r read-web-achive} -->
<!-- staticgen_arch17 <- read_html(staticgen_tm$link[17]) -->
<!-- staticgen_arch17 %>%  -->
<!--   html_nodes("h4") %>%  -->
<!--   html_text() -->
<!-- ``` -->

<!-- ```{r test-read-html} -->
<!-- staticgen_list <- as.list(1:length((staticgen_clean))) -->
<!-- for (i in 1:5) { -->
<!--   read_html("https://www.staticgen.com/") -->
<!-- } -->

<!-- staticgen_2013 <- read_html(staticgen_clean$link[1]) -->
<!-- staticgen_2014 <- read_html(staticgen_clean$link[2]) -->
<!-- staticgen_2015 <- read_html(staticgen_clean$link[3]) -->
<!-- staticgen_2016 <- read_html(staticgen_clean$link[4]) -->
<!-- staticgen_2017 <- read_html(staticgen_clean$link[5]) -->
<!-- staticgen_2018 <- read_html(staticgen_clean$link[6]) -->
<!-- staticgen_2019 <- read_html(staticgen_clean$link[7]) -->

<!-- staticgen_wbpg <- list( -->
<!--   w2013 = staticgen_2013, -->
<!--   w2014 = staticgen_2014, -->
<!--   w2015 = staticgen_2015, -->
<!--   w2016 = staticgen_2016, -->
<!--   w2017 = staticgen_2017, -->
<!--   w2018 = staticgen_2018, -->
<!--   w2019 = staticgen_2019 -->
<!--     ) -->

<!-- saveRDS(staticgen_wbpg, file = "data/staticgen_wbpg.rds") -->

<!-- staticgen_wbpg$w2019 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->

<!-- saveRDS(staticgen_2013, file = "data/staticgen_2013.rds") -->
<!-- saveRDS(staticgen_2014, file = "data/staticgen_2014.rds") -->
<!-- saveRDS(staticgen_2015, file = "data/staticgen_2015.rds") -->
<!-- saveRDS(staticgen_2016, file = "data/staticgen_2016.rds") -->
<!-- saveRDS(staticgen_2017, file = "data/staticgen_2017.rds") -->
<!-- saveRDS(staticgen_2018, file = "data/staticgen_2018.rds") -->
<!-- saveRDS(staticgen_2019, file = "data/staticgen_2019.rds") -->

<!-- staticgen_2019 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->


<!-- staticgen_wbpg <- lapply(staticgen_clean$link, read_html) -->


<!-- test_read <- as_tibble(replicate(5, "https://www.staticgen.com/")) -->
<!-- t <- lapply(test_read$value, read_html) -->
<!-- names(t) <- paste("a", 1:length(t), sep = "") -->


<!-- staticgen_list <- vector("list", 5) -->

<!-- # http://bit.ly/2Ytz4L7 -->
<!-- x <- as.list(1:5) -->
<!-- names(x) <- paste("a", 1:length(x), sep = "") -->
<!-- list2env(x , envir = .GlobalEnv) -->

<!-- millenium <- function(n) { -->
<!--   n <- n - 2000 -->
<!--   return(n) -->
<!-- } -->

<!-- ``` -->

