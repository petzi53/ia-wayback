---
title: "Scrapping Archive Data With The Wayback Machine"
author: "Peter Baumgartner"
date: "`r Sys.Date()`"
output:
  html_notebook:
    fig_caption: yes
    number_sections: yes
    pandoc_args: --number-offset=0,0
    toc: yes
    toc_depth: 4
  word_document:
    toc: yes
    toc_depth: '4'
  pdf_document:
    pandoc_args: --number-offset=0,0
    toc: yes
    toc_depth: '4'
    latex_engine: xelatex
  github_document:
    toc: yes
    toc_depth: 4
  html_document:
    fig_caption: yes
    keep_md: yes
    number_sections: yes
    pandoc_args: --number-offset=0,0
    toc: yes
    toc_depth: 4
---

# Setup

```{r label = "global-options", highlight=TRUE}
knitr::opts_chunk$set(
        message = F,
        error = F,
        warning = F,
        comment = NA,
        highlight = T,
        prompt = T
        )
### Set the global option options(stringsAsFactors = FALSE) 
### inside a parent function and restore the option after the parent function exits
if (!require("xfun"))
        {install.packages("xfun", repos = 'http://cran.wu.ac.at/')
        library(xfun)}
xfun::stringsAsStrings()


### install and load some important packages
### https://github.com/tidyverse/tidyverse
if (!require("tidyverse"))
        {install.packages("tidyverse", repos = 'http://cran.wu.ac.at/')
        library(tidyverse)}

### above command installed and loaded the core tidyverse packages:
# ggplot2:    data visualisation
# tibble:     a modern take on data frames
# tidyr:      data tidying
# readr:      data import (csv, tsv, fwf)
# purrr:      functional R programming
# dplyr:      data (frame) manipulation
# stringr:    string manipulation
# forcats:    working with categorial varialbes
# tidyselect: backend for the selecting functions of the 'tidyverse'. (?, new?)


### My personal reminder for other important packages:
### Working with times:
# hms, for times.

# lubridate, for date/times.
if (!require("lubridate"))
        {install.packages("lubridate", repos = 'http://cran.wu.ac.at/')
        library(lubridate)}

### Importing other types of data:
# feather, for sharing with Python and other languages.
# haven, for SPSS, SAS and Stata files.
# httr, for web apis.
# jsonlite for JSON.
# readxl, for .xls and .xlsx files.


# rvest, for web scraping.
if (!require("rvest"))
        {install.packages("rvest", repos = 'http://cran.wu.ac.at/')
        library(rvest)}


# xml2, for XML.
if (!require("xml2"))
        {install.packages("xml2", repos = 'http://cran.wu.ac.at/')
        library(xml2)}

### Modelling
# modelr, for modelling within a pipeline
# broom, for turning models into tidy data


### Special packages for this article
if (!require("wayback"))
        {remotes::install_github("hrbrmstr/wayback", build_vignettes = TRUE)
        library(tidyverse)}
```

# Preliminaries

## Does the Internet Archive have my research URL cached?

Using `archive_available(url, timestamp)`: Timestamp is optional. 
The function returns a tibble with one observation and 5 variables:

```{r url-cached}
staticgen_avail <- archive_available("https://www.staticgen.com/")
staticgen_avail
```

## Retrieve site mementos from the Internet Archive

Mementos are prior versions of web pages that have been cached from web crawlers. They can be found in web archives (such as the Internet Archive) or systems that support versioning such as wikis or revision control systems.

With `get_mementos(url, timestamp = format(Sys.Date(), "%Y"))` we will receive a short list of relevant links to the archived content. The function returns the four link relation types as in the [Request for Comment for the Memento framework](https://mementoweb.org/guide/rfc/#Link-Header-Relation-Types) outlined.

1. Link Relation Type "original"
2. Link Relation Type "timemap"
3. Link Relation Type "timegate"
4. Link Relation Type "memento"

Besides these 4 main types of link relations the function provides also the first, previous and last available memento, which is normally identical with the memento link relation type. In addition to the two columns `link' and `rel` there is a third one `ts`, containing the time stamps (empty for the first three link relation types). The return value in total is a tibble with 7 observations (rows) and three columns.

```{r get-mementos}
staticgen_mntos <- get_mementos("https://www.staticgen.com/")
staticgen_mntos
```

## Get the point-in-time memento crawl list

Providing an URL in the search field of the Wayback Machine results in the interactive browser version into the [calender view](https://web.archive.org/web/*/https://www.staticgen.com) where the dates with archived content are blue or green (redirected URL) circled. the bigger the circles the more snapshots were archived on these dates.

We get these dated crawl list with the second observation of the `get_mementos` function.

```{r get-timemap}
staticgen_tm <- get_timemap(staticgen_mntos$link[2])
staticgen_tm
```

In addition to the 476 captures of the interactive browser version there are four more rows (480), relating to the four link relation types mentioned above. The last line is empty.

## Summary: Putting all together

We can put together all three preliminay steps into a function `get_rawcrawl(url)`. This functions gets an URL and returns a list of all archived versions for this URL.

1. Check if for the URL exists an archived version. If not: stop execeution.
2. If exists an archived version, then retireve mementos for this url from the Internet Archive.
3. Get the point-in-time memento crawl list for this URL

```{r get-raw-crawl}
get_rawcrawl <- function(url) {
  if(is_url(url)) {
    url_archived <- archive_available(url) 
    if (url_archived$available) {
      mementos <- get_mementos(url)
      time_map <- get_timemap(mementos$link[2])
      return(time_map)
    } else {
      return(paste0("There exists no archive of '", url, "'."))
    }
  } else {
    stop("The functions needs a valid URL format: 'http://' or 'https://'")
  }
}

is_url <- function(s) {
  class(s) == "character" && (substr(s,1,7) == "http://" || substr(s,1,8) == "https://")
}

sg_rawcrawl <- get_rawcrawl("https://www.staticgen.com/")
saveRDS(sg_rawcrawl, file = "data/sg_rawcrawl.rds")
sg_rawcrawl
```

# Tidy data
## Introduction

At first we have to clean up our data frame of URLs to crawl. Tidying the timemap data frame is a multipe step procedure:

+ Clean up so that only memento links remain
+ Delete unnecessary rows `type` and `from`.
+ Convert row `datetime` from class 'character' to class 'datetime'.
+ Delete duplicate datetime records. (Sometimes there are more than one capture taken at the same day, refering to the URL and the port used.)
+ Filter rows with an algorithm, so that only those mementos remain which are suitable for the comparison analysis. For instance: Take the first memento for every year, or every month etc.

The last step is special as it requires a decision by the author or analyist of the data. 

## General data cleaning

```{r get-clean-crawl}
get_cleancrawl <- function(df) {
  df$datetime <- as.POSIXct(df$datetime, format = "%a, %d %b %Y")
  df_crawl <- df %>% 
      filter(rel == "memento") %>%
      select(link, datetime) %>%
      distinct(datetime, .keep_all = TRUE)  # delete duplicate datetime
  return(df_crawl)
}

sg_rawcrawl <- readRDS("data/sg_rawcrawl.rds")
sg_cleancrawl <- get_cleancrawl(sg_rawcrawl)
saveRDS(sg_cleancrawl, file = "data/sg_cleancrawl.rds")
sg_cleancrawl
```

## Filter crawl list

This is a more complicated functions as it provides several posssiblities:

1. Limit comparison period:
    1. Choose start of comparison period by row number.
    2. Choose start of comparison period by (nearest) date.
    3. Choose end of comparison period by row number.
    4. Choose end of comparison period by (nearest) date.
2. Adding URLs in any case, independent of the chosen filter algorithm:
    1. Add URL of the last memento in any case, independent of the filter option.
    2. Add URL of the live web site with datetime of today to the end of the data frame.
3. Mode of calculation of the chosen algorithm:
    1. Take always first entry of the chosen period.
    2. Take always last entry of the chosen period.
    3. Take datetime of your first chosen memento to calculate the period.
4. Filter rows with one of the following options:
    1. **Year:** One URL to crawl for every year.
    2. **Half year (six months):** One URL to crawl for every 6 months.
    3. **Quarter (three months):** One URL to crawl for every quarterly period. 
    4. **Month:** One URL to craw for every month.
    5. **Number:** Filter <number> URLs with roughly equidistance of time.

Limiting the comparison period is useful for several reasons:
    + To limit the time period for the data analysis.
    + Ignore the first mementos of an archived web site because they have not enough information.
    + Generate various collections of mementos, depending of their different structure to crawl.

At the moment I have only implemented: 1.1, 2.2, 3.1, and 4.1.

```{r filter-craw-list}
get_crawllist <- function(
  df, start=1, end=nrow(df),
  last_mnto=FALSE, live_url="",
  choose_mnto='first',
  filter_mntos='year') {
  
  if (start != 1) {df <- tail(df, -start)}
  
  if (filter_mntos == 'year') {
    crawl_list <- df %>%
    mutate(year = year(datetime)) %>%
    # http://bit.ly/2K0oho0
    group_by(year) %>%
    filter(datetime == min(datetime)) %>% 
    ungroup()
  }
  if (!live_url == '') {
    live_wbpg <- tibble(link = live_url, 
                        datetime = as_datetime(today()), 
                        year = as.numeric(2019))
    crawl_list <- data.frame(rbind(crawl_list, live_wbpg))
  }
  return(crawl_list)
}


sg_cleancrawl <- readRDS("data/sg_cleancrawl.rds")
# The first 12 mementos have a different web site structure
# I am loosing about 8 month for the comparison
sg_crawllist <- get_crawllist(sg_cleancrawl, start = 13, live_url = "https://www.staticgen.com/")
saveRDS(sg_crawllist, file = "data/sg_crawllist.rds")
sg_crawllist

```

```{r list-xml-docs}

# bit.ly/SO-save-xml
roundtrip <- function(obj) {
  tf <- tempfile()
  con <- file(tf, "wb")
  on.exit(unlink(tf))

  xml_serialize(obj, con)
  close(con)
  con <- file(tf, "rb")
  on.exit(close(con), add = TRUE)
  xml_unserialize(con)
}


# create a list of xml documents
sg_crawllist <- readRDS("data/sg_crawllist.rds")
my_xml <- read_html(sg_crawllist$link[1])

sg_wbpg <- lapply(sg_crawllist$link, read_html)

test_wbpg <- sg_wbpg
my_data <- test_wbpg
con <- file("data/my_data", "wb")
serialize(my_data, con)
y <- roundtrip(test_wbpg[[1]])

```


# Web page crawl

## Introduction

We have stored the web pages as xml-documents in `sg_wbpg` and are now able to retrieve the relevant data for our analysis. Using the code inspector of Google Chrome we will eventually find two important items to retrieve content for our analysis.

But there is a wicked problem: It turns out that the structure of the website has serveral times changed. So we have to inspect every instance of `sg_crawlist` interactively to detect how we can retrieve the data for our analysis. This means we have to go to the archived webpage and apply Google Chrome inspector to find out the HTML/XML node we have to apply. We can test our findings with the appropriate subsetting of `sg_wbpg`. 

## HTML structure of websites

It turns out that for all mementos the `h4` tag will produce the name of the static website generator.

For the data values the situation is more complicated:

+ **Between 2014 and 2018** the data will scraped with the CSS class selector `.stats`. It produces for every name three values: repo stars, open issues, and repo forks.
+ **2019** needs the CSS class selector `.OpenSourceStat-fXFkTK`. This produces for every name four values: repo stars, open issues, repo forks, and twitter followers. Every value is followed by a `+` or `-` sign and the number of changes since the last update of the website. I delete the figure of changes as the last update is not relevant for my analysis, respectively my data will show the changes between the dates of the mementos retrieval.
+ **Live website** has with `OpenSourceStat-sc-1jlkb1d-2` still another CSS class selector. I am somewhat worried that future updates of the website will always have different CSS class selectors. It seems that the random endings after the dash are generated automatically by the Content Management Sysgtem (CMS).  

## Dataframe with specific retrieval information

For the main function `get_content` I will store specific retireval information with the following items:

+ Internet Archive link to crawl (from `sg_crawllist`)
+ datetime (from `sg_crawllist`)
+ Tag to retrieve the names ("h4")
+ CSS class to retrieve the data (".stat", ".OpenSourceStat-fXFkTK" and ".OpenSourceStat-sc-1jlkb1d-2")
+ Number of data items to build different columns
+ Specific information for cleaning up the data itmes (regex: "[:digit:]+")

```{r sg-retrival}
sg_crawllist <- readRDS("data/sg_crawllist.rds")
get_names <- rep("h4", nrow(sg_crawllist))
get_data <- c(rep(".stat", 5), ".OpenSourceStat-fXFkTK", ".OpenSourceStat-sc-1jlkb1d-2")
get_cols <- c(rep(3L, 5), 4L, 4L)
get_regex <- c(rep("", 5), rep("[:digit:]+",2))
sg_retrieval <- cbind(sg_crawllist[1:2], get_names, get_data, get_cols, get_regex)
names(sg_retrieval) <- c("link", "datetime", "name", "data", "n_cols", "regex")
saveRDS(sg_retrieval, file = "data/sg_retrieval.rds")
```

# Analysis (stub)

I will concentrate the analysis on four issues:

1. Number of static web site generators for every memento.
2. Name of the static generator.
3. Number of stars for the repository as a proxy for its popularity.
4. Number of repository forks as a proxy for the size of the developer community.

Other data like open issues and followers on twitter are not included as they have in my opinion only a weak relationship with the dissemination of the web site generator. Perhaps the exclusion of twitter followers needs more reflection:

+ The display of the number of twitter followers started only around 2019.
+ As today (2019-07-31) only 37 static website generators have a twitter account.
+ Even leading static website frameworks (e.g., Next.js) have no twitter account.
+ The number of followers results not only from the popularity of the generator but also from interesting and well written tweeds.


## Memento 2014


```{r memento-2014}
# mem <- read_memento(sg_crawllist$link[1])
# sg_test <- readRDS("data/sg_wbpg.rds")
# load("data/sg_webpage.Rdata")
# attach("data/sg_webpage.Rdata")

xml_doc <- my_xml

test_wbpg <- sg_wbpg

write_xml(test_wbpg[[1]], "data/test_html", options = "format" )
t <- readLines("data/test_html")

con <- file("data/test1_xml", open = "wb")
xml_serialize(test_wbpg[[1]], con)
flush(con)
close(con)


con <- file("data/test1_xml", open = "rb")
on.exit(close(con), add = TRUE)
test1_xml <- xml_unserialize(con)
memento2014 <- t
memento2014 %>%
  html_nodes("h4") %>%
  html_text()
memento2014  %>%
  html_nodes(".stat") %>%
  html_text()
```

<!-- ## Memento 2015 -->

<!-- ```{r memento-2015} -->
<!-- memento2015 <- sg_wbpg[[2]] -->
<!-- memento2015 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- memento2015  %>%  -->
<!--   html_nodes(".stat") %>% -->
<!--   html_text() -->
<!-- ``` -->

<!-- ## Memento 2016 -->

<!-- ```{r memento-2016} -->
<!-- # mem <- read_memento(sg_crawllist$link[1]) -->
<!-- memento2016 <- sg_wbpg[[3]] -->
<!-- memento2016 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- memento2016  %>%  -->
<!--   html_nodes(".stat") %>% -->
<!--   html_text() -->
<!-- ``` -->

<!-- ## Memento 2017 -->

<!-- ```{r memento-2017} -->
<!-- memento2017 <- sg_wbpg[[4]] -->
<!-- memento2017 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- memento2017  %>%  -->
<!--   html_nodes(".stat") %>% -->
<!--   html_text() -->
<!-- ``` -->

<!-- ## Memento 2018 -->

<!-- ```{r memento-2018} -->
<!-- memento2018 <- sg_wbpg[[5]] -->
<!-- memento2018 %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- memento2018  %>%  -->
<!--   html_nodes(".stat") %>% -->
<!--   html_text() -->
<!-- ``` -->

<!-- ## Memento 2019 -->

<!-- ```{r memento-2019} -->
<!-- memento2019 <- sg_wbpg[[6]] -->
<!-- memento2019 %>%  -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- memento2019  %>%  -->
<!--   html_nodes(".OpenSourceStat-fXFkTK") %>% -->
<!--   html_text() -->
<!-- ``` -->

<!-- ## Memento Live -->


<!-- ```{r live-website} -->
<!-- memento_live <- sg_wbpg[[7]] -->
<!-- memento_live %>% -->
<!--   html_nodes("h4") %>% -->
<!--   html_text() -->
<!-- memento_live  %>%  -->
<!--   html_nodes(".OpenSourceStat-sc-1jlkb1d-2") %>% -->
<!--   html_text() -->
<!-- ``` -->


# Producing data frames with the retrieved data

To retrieve the content from the web pages stored in sg_retrieval we can use the following code:

```{r get-content}
# sg_wbpg <- readRDS("data/sg_wbpg.rds") does not work!!!!!!
sg_retrieval <- readRDS("data/sg_retrieval.rds")
# remember that I set global option `stringsAsFactors = FALSE`. 
# See chunk `global-option` at the beginning of this file

get_content <- function(xml_document, df_retrieval) {
  sg_list = list()
  for (i in 1:length(xml_document)) {
    
    # prepare specific retrieval modes
    xml_doc <- xml_document[[i]]
    name_node <- df_retrieval$name[i]
    data_node <- df_retrieval$data[i]
    extract_data <- df_retrieval$regex[i]
    n_cols <- df_retrieval$n_cols[i]
    
    # retrieve data
    sg_names <- xml_doc %>%
      html_nodes(name_node) %>%
      html_text()
    sg_data <- xml_doc %>% 
      html_nodes(data_node) %>%
      html_text()
    # delete second part of the string, starting with '+'
    # these changes of days from the last update of web sites are not relevant for my analysis,
    # because I am using the figures from the timed mementos
    if (extract_data != '') {
      sg_data <- as.integer(stringr::str_extract(sg_data, extract_data))
    }
    # convert character string to data frame with 3 or 4 columns. See: bit.ly/SO-vec-to-df
    sg_data <- data.frame(matrix(sg_data, ncol = n_cols, byrow = TRUE))
    sg_data <- data.frame(cbind(sg_names, sg_data))
    if (n_cols == 3) {
      names(sg_data) <- list("name", "repo_stars", "open_issues", "repo_forks")
    }
    if (n_cols == 4) {
      names(sg_data) <- list("name", "repo_stars", "open_issues", "repo_forks", "twitter_followers")
    }
    
    # append dataframe to end of the data_list
    sg_list[[i]] <- sg_data
  }
  return(sg_list)
}

sg_data_collection <- get_content(sg_wbpg, sg_retrieval)
saveRDS(sg_data_collection, file = "data/sg_data_collection.rds")



```


